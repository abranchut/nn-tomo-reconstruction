{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A script to reconstruct a MOF particle from a subset of 8 projections from HSH75-Pd-HAADF_hdr0.ali (this code works for .mrc and .ali files), which originally is a stack of 29 projections between -70° and 70°.\n",
    "\n",
    "For an introduction on Python classes, see tutorial_classes.ipynb.\n",
    "For an introduction on how neural networks works, see https://www.3blue1brown.com/topics/neural-networks.\n",
    "For more details about the training procedure and the overall pytorch framework, see https://pytorch.org/tutorials/beginner/basics/intro.html.\n",
    "\n",
    "For more details about a particular function / method, see the specific documentation in the code.\n",
    "\n",
    "For more details about NN-FBP implementation, see the original article: Fast Tomographic Reconstruction from Limited Data Using Artificial Neural\n",
    "Networks, D. M. Pelt and al., 2013 (https://ieeexplore.ieee.org/document/6607157)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "from nntomo.utilities import get_MSE_loss\n",
    "from nntomo.network import nnfbp_training\n",
    "from nntomo.nnfbp import DatasetNNFBP\n",
    "from nntomo.projection_stack import ProjectionStack\n",
    "from nntomo.volume import Volume\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "The original 29 projections from STEM measurements. The         A subset of 8 projections: a comparison will be made between\n",
    "SIRT reconstruction using all these projections will be         the NN-FBP and the SIRT reconstructions to evaluate the\n",
    "used as a reference for the real volume of the particule.       reconstruction performance for very small subsets of projections.\n",
    "\n",
    "<img src=\"data/gifs/particule_29proj.gif\" width=\"300\"/>                                  <img src=\"data/gifs/particule_8proj.gif\" width=\"300\"/>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a stack of ellipses and its associated projections are used for the training dataset. The training dataset is used to modify the values of the weights and biases of the network, by comparing the predicted output of each input of the dataset and the real outputs. The projections are made with the ASTRA toolbox. The corresponding dataset is created and saved. See the pytorch tutorial for more information about the use of Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation of the ellipses: [████████████████████████████████████████████████████████████] 100/100 Est wait 00:0.0\n",
      "\n",
      "Saving volume...\n",
      "File saved at c:\\Users\\Admin-tomo\\Documents\\tomo-reconstruction-alix\\github_repository\\nn-tomo-reconstruction\\scripts\\data\\volume_files\\rand7ellipses1024.mrc.\n",
      " ID: rand7ellipses1024\n"
     ]
    }
   ],
   "source": [
    "# Generation of the ellipses\n",
    "ellipses_volume = Volume.stack_7ellipses(100, shape=1024, semi_axis_range=(20,200), padding=100)\n",
    "ellipses_volume.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset...\n",
      "File saved at c:\\Users\\Admin-tomo\\Documents\\tomo-reconstruction-alix\\github_repository\\nn-tomo-reconstruction\\scripts\\data\\datasets_files\\rand7ellipses1024-full9th_rand7ellipses1024_bin.pickle.\n",
      " ID: rand7ellipses1024-full9th_rand7ellipses1024_bin\n"
     ]
    }
   ],
   "source": [
    "# Creation of a volume object for the ellipses\n",
    "ellipses_volume = Volume.retrieve('rand7ellipses1024')\n",
    "\n",
    "# Creation of the projections (9 projections from -90° to 70°)\n",
    "ellipses_projections = ProjectionStack.from_volume(ellipses_volume, 9, 'full')\n",
    "\n",
    "# Creation of the dataset for the neural network training:\n",
    "training_dataset = DatasetNNFBP(ellipses_projections, ellipses_volume)\n",
    "\n",
    "# Saving of the dataset:\n",
    "training_dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Each slice of the ellipses stack:                          The 9 projections taken around the axis perpendicular to the screen:\n",
    "\n",
    "<img src=\"data/gifs/ellipses.gif\" width=\"300\"/>              <img src=\"data/gifs/ellipses_proj.gif\" width=\"800\"/>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, random spheres are used for the validation set. The validation set is used to assess the performance of the network during the training phase on unknown data, and decide when to stop the training to avoid overfitting. The projections are also made with the ASTRA toolbox. The corresponding dataset is created and saved. See the pytorch tutorial for more information about the use of Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation of the spheres: [████████████████████████████████████████████████████████████] 60/60 Est wait 00:0.00\n",
      "\n",
      "Saving volume...\n",
      "File saved at c:\\Users\\Admin-tomo\\Documents\\tomo-reconstruction-alix\\github_repository\\nn-tomo-reconstruction\\scripts\\data\\volume_files\\randspheres1024.mrc.\n",
      " ID: randspheres1024\n"
     ]
    }
   ],
   "source": [
    "# Generation of the spheres\n",
    "spheres = Volume.random_spheres(60, shape=1024, radius_range=(5,100), padding=50)\n",
    "spheres.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset...\n",
      "File saved at c:\\Users\\Admin-tomo\\Documents\\tomo-reconstruction-alix\\github_repository\\nn-tomo-reconstruction\\scripts\\data\\datasets_files\\randspheres1024-full9th_randspheres1024_bin.pickle.\n",
      " ID: randspheres1024-full9th_randspheres1024_bin\n"
     ]
    }
   ],
   "source": [
    "# Creation of a volume object for the spheres\n",
    "spheres = Volume.retrieve('randspheres1024')\n",
    "\n",
    "# Creation of the projections (9 projections from -90° to 70°)\n",
    "spheres_projections = ProjectionStack.from_volume(spheres, 9, 'full')\n",
    "\n",
    "# Creation of the dataset for the neural network training:\n",
    "validation_dataset = DatasetNNFBP(spheres_projections, spheres)\n",
    "\n",
    "# Saving of the dataset:\n",
    "validation_dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "The random spheres:                                                  The 9 projections:\n",
    "\n",
    "<img src=\"data/gifs/random_spheres.gif\" width=\"300\"/>                              <img src=\"data/gifs/randspheres_proj.gif\" width=\"300\"/>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the neural network using the computed datasets. The network informations are automatically saved every 30s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training.\n",
      "Epoch 1 (n=0)\n",
      "-------------------------------\n",
      "Avg MSELoss(): 0.033581 \n",
      "\n",
      "Epoch 2 (n=0)\n",
      "-------------------------------\n",
      "Avg MSELoss(): 0.015099 \n",
      "\n",
      "Epoch 3 (n=0)\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m Nh \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Training of NN-FBP\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m network \u001b[38;5;241m=\u001b[39m \u001b[43mnnfbp_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\nntomo\\network.py:160\u001b[0m, in \u001b[0;36mnnfbp_training\u001b[1;34m(train_dataset, valid_dataset, Nh, batch_size, learning_rate, Nstop, max_epoch, custom_id)\u001b[0m\n\u001b[0;32m    156\u001b[0m     network_id \u001b[38;5;241m=\u001b[39m custom_id\n\u001b[0;32m    158\u001b[0m model \u001b[38;5;241m=\u001b[39m NNFBP(train_dataset, Nh, network_id)\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\nntomo\\network.py:101\u001b[0m, in \u001b[0;36mmodel_training\u001b[1;34m(model, train_dataset, valid_dataset, device, batch_size, learning_rate, Nstop, max_epoch, network_id)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m Nstop:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (n=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 101\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     loss \u001b[38;5;241m=\u001b[39m test_loop(valid_dataloader, model, loss_fn)\n\u001b[0;32m    103\u001b[0m     checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_states_history\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(deepcopy(model\u001b[38;5;241m.\u001b[39mstate_dict()))\n",
      "File \u001b[1;32mc:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\nntomo\\network.py:19\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\nntomo\\nnfbp.py:52\u001b[0m, in \u001b[0;36mNNFBP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_sigm_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Dataset retrieving using the ids\n",
    "training_dataset = DatasetNNFBP.retrieve(\"rand7ellipses1024-full9th_rand7ellipses1024_bin\")\n",
    "validation_dataset = DatasetNNFBP.retrieve(\"randspheres1024-full9th_randspheres1024_bin\")\n",
    "\n",
    "# Number of hidden nodes in the network\n",
    "Nh = 8\n",
    "\n",
    "# Training of NN-FBP\n",
    "network = nnfbp_training(training_dataset, validation_dataset, Nh, max_epoch=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstuctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of the reconstruction of the particule with 8 projections, using the network, and the SIRT algorithm. A quantitative comparison between NN-FBP and SIRT can then be made by computing the MSE loss of the reconstruction using a SIRT reconstruction with the 29 projections as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\mrcfile\\mrcinterpreter.py:206: RuntimeWarning: Map ID string not found - not an MRC file, or file is corrupt\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "c:\\Users\\Admin-tomo\\anaconda3\\envs\\abtem_env\\Lib\\site-packages\\mrcfile\\mrcinterpreter.py:216: RuntimeWarning: Unrecognised machine stamp: 0x00 0x00 0x00 0x00\n",
      "  warnings.warn(str(err), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# The 29 projections stack of the MOF particule\n",
    "mof_projections_file = \"data/projection_files/HSH75-Pd-HAADF_hdr0.ali\"\n",
    "mof_29proj = ProjectionStack.from_mrc_file(mof_projections_file, 'tem')\n",
    "\n",
    "# The stack with a subset of 8 projections\n",
    "mof_8proj = mof_29proj.get_proj_subset(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of NN reconstruction.\n",
      "Reconstruction part 1/2: [████████████████████████████████████████████████████████████] 8/8 Est wait 00:0.010\n",
      "\n",
      "Reconstruction part 2/2: [████████████████████████████████████████████████████████████] 8/8 Est wait 00:0.090\n",
      "\n",
      "Saving volume...\n",
      "File saved at c:\\Users\\Admin-tomo\\Documents\\tomo-reconstruction-alix\\github_repository\\nn-tomo-reconstruction\\scripts\\data\\volume_files\\nn_rand7ellipses1024-full9th_rand7ellipses1024_bin_8h_HSH75-Pd-HAADF_hdr0-sub8.mrc. ID: nn_rand7ellipses1024-full9th_rand7ellipses1024_bin_8h_HSH75-Pd-HAADF_hdr0-sub8\n"
     ]
    }
   ],
   "source": [
    "# NN-FBP reconstruction for the 8 projections stack\n",
    "nn_reconstr_8proj = mof_8proj.get_NNFBP_reconstruction(network)\n",
    "nn_reconstr_8proj.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of SIRT reconstruction.\n",
      "Saving volume...\n",
      "File saved at c:\\Users\\Admin-tomo\\Documents\\tomo-reconstruction-alix\\github_repository\\nn-tomo-reconstruction\\scripts\\data\\volume_files\\sirt150_HSH75-Pd-HAADF_hdr0-sub8.mrc. ID: sirt150_HSH75-Pd-HAADF_hdr0-sub8\n"
     ]
    }
   ],
   "source": [
    "# SIRT reconstructions for the 8 projections stack, with 150 iterations\n",
    "sirt_reconstr_8proj = mof_8proj.get_SIRT_reconstruction(150)\n",
    "sirt_reconstr_8proj.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of SIRT reconstruction.\n",
      "Saving volume...\n",
      "File saved at c:\\Users\\Admin-tomo\\Documents\\tomo-reconstruction-alix\\github_repository\\nn-tomo-reconstruction\\scripts\\data\\volume_files\\sirt150_HSH75-Pd-HAADF_hdr0.mrc. ID: sirt150_HSH75-Pd-HAADF_hdr0\n"
     ]
    }
   ],
   "source": [
    "# SIRT reference reconstruction with 150 iterations\n",
    "sirt_reconstr_29proj = mof_29proj.get_SIRT_reconstruction(150)\n",
    "sirt_reconstr_29proj.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "The reference reconstruction,                  NN-FBP reconstruction (8 projections):                SIRT reconstruction (8 projections):\n",
    "SIRT with 29 projections:\n",
    "\n",
    "<img src=\"data/gifs/particule_sirt29.gif\" width=\"300\"/>            <img src=\"data/gifs/particule_nn8.gif\" width=\"300\"/>            <img src=\"data/gifs/particule_sirt8.gif\" width=\"300\"/>\n",
    "isosurface value: 190/255                          isosurface value: 245/255                          isosurface value: 170/255\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation and MSE computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation: all voxel values are set to 0 or 1, depending on a segmentation value. This value is set arbitrarily, by looking at the shape of the\n",
    "# volume in imod (isosurface value).\n",
    "nn_reconstr_8proj = nn_reconstr_8proj.get_segmented_volume(245/255)\n",
    "sirt_reconstr_8proj = sirt_reconstr_8proj.get_segmented_volume(170/255)\n",
    "sirt_reconstr_29proj = sirt_reconstr_29proj.get_segmented_volume(190/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN-FBP MSE: 0.0001929197460412979\n",
      "SIRT MSE: 0.0002627996727824211\n"
     ]
    }
   ],
   "source": [
    "# MSE computation\n",
    "print(f\"NN-FBP MSE: {get_MSE_loss(sirt_reconstr_29proj, nn_reconstr_8proj)}\")\n",
    "print(f\"SIRT MSE: {get_MSE_loss(sirt_reconstr_29proj, sirt_reconstr_8proj)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.125\n",
      "4096.0\n",
      "24.078125\n",
      "36.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cupy\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated()/1024**2)\n",
    "print(torch.cuda.memory_reserved()/1024**2)\n",
    "print(cupy.get_default_memory_pool().used_bytes()/1024**2)\n",
    "print(cupy.get_default_memory_pool().total_bytes()/1024**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abtem_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
